---
title: "statistical rethinking"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#sudo apt-get install libssh2-1-dev1
#install.packages(c("coda","mvtnorm","devtools","loo"))

library(rstan)
library(shinystan)
#library(devtools)
#devtools::install_github("rmcelreath/rethinking", force=TRUE)

library(rethinking)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

```

## 
```{r}
p_grid <- seq(from=0, to=1, length.out=1000)
prob_p <- rep(1, 1000)
prob_data <- dbinom(6,  size=9, prob=p_grid)
posterior <- prob_data * prob_p
posterior <- posterior / sum(posterior)
samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
# posterior predictions
nw <- rbinom(1e4, size=9, prob=samples)
```

PI equal area in each tail
HPDI (highest posterrior density intervals) narrowest interval containing mass

## Chapter 1
1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.

```{r HW week 1}
p_grid <- seq(from=0, to=1, length.out=1000)
prob_p <- rep(1, 1000)
prob_data <- dbinom(8,  size=15, prob=p_grid)
posterior <- prob_data * prob_p
posterior <- posterior / sum(posterior)
samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
# posterior predictions
nw <- rbinom(1e4, size=9, prob=samples)
dens(samples)
```
2. Start over in 1, but now use a prior that is zero below p = 0:5 and a constant
above p = 0:5. This corresponds to prior information that a majority
of the Earthâ€™s surface is water. What difference does the better prior make?
If it helps, compare posterior distributions (using both priors) to the true
value p = 0:7.

```{r}
p_grid <- seq(from=0, to=1, length.out=1000)
prob_p <- c(rep(0, 500), rep(1, 500))
prob_data <- dbinom(6,  size=9, prob=p_grid)
posterior <- prob_data * prob_p
posterior <- posterior / sum(posterior)
samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
# posterior predictions
nw <- rbinom(1e4, size=9, prob=samples)
dens(samples)
```


Grid Approximation
```{r}
mu.list <- seq(from=140, to=160, length.out=2000)
sigma.list <- seq(from=4, to=9, length.out=2000)
post <- expand.grid(mu=mu.list, sigma=sigma.list)
# expand.grid creates cartesian product but flattened
post$LL <- sapply(1:nrow(post), function(i) sum(dnorm(d2$height, mean=post$mu[i], sd=post$sigma[i], log=TRUE)))
post$prod <- post$LL + dnorm(post$mu, 178, 20, log=TRUE) + dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))
```

```{r}
contour_xyz(post$mu, post$sigma, post$prob)
image_xyz(post$mu, post$sigma, post$prob)
```

Sample from posterior
note that posterior for mu and sigma is close to gaussian
```{r}
# remember prob is 'tall' : 1 row for each mu, sigma
sample.rows <- sample(1:nrow(post), size=1e4, replace=TRUE, prob=post$prob)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
plot(sample.mu, sample.sigma, cex=0.5, pch=16, col=col.alpha(rangi2,0.1))
dens(sample.mu, norm.comp=TRUE); 

dens(sample.sigma, norm.comp=TRUE)

HPDI(sample.mu)
HPDI(sample.sigma)
```
rerun with only 20 samples
mean is still gaussian.  Why? no impact of variance distribution
sd not Gaussian 

```{r}
d3 <- sample(d2$height, size=20)
mu.list <- seq(from=150, to=170, length.out=2000)
sigma.list <- seq(from=4, to=20, length.out=2000)
post2 <- expand.grid(mu=mu.list, sigma=sigma.list)
# expand.grid creates cartesian product but flattened
post2$LL <- sapply(1:nrow(post2), function(i) sum(dnorm(d3, mean=post2$mu[i], sd=post2$sigma[i], log=TRUE)))
post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, log=TRUE) + dunif(post2$sigma, 0, 50, TRUE)
post2$prob <- exp(post2$prod - max(post2$prod))
sample.rows <- sample(1:nrow(post2), size=1e4, replace=TRUE, prob=post2$prob)
sample.mu <- post2$mu[sample.rows]
sample.sigma <- post2$sigma[sample.rows]
plot(sample.mu, sample.sigma, cex=0.5, pch=16, col=col.alpha(rangi2,0.1))
dens(sample.mu, norm.comp=TRUE); 

dens(sample.sigma, norm.comp=TRUE)

HPDI(sample.mu)
HPDI(sample.sigma)

```

Here we assume the heights are normally distributed
normal prior for mu, so normal posterior
heights are assumed iid. Could argue conservative assumption given ignorance.
Note that posterior map values close to ML.  Because flat priors and large data set
We also look at impact of prior with tighter Gaussian 0.1 sd
now posterior stays very close to prior for mu
and variance increases to compensate for tight prior for mu

NB can view prior as previous data: prior (mu, sigma^2) same as n samples with with mean mu
TODO!!
```{r code 4.24}
library(rethinking)
data(Howell1)
d <- Howell1
# adults
d2 <- d[d$age >= 18, ]
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)
m4.1 <- map(flist, data=d2)
# plot priors
curve(dnorm(x, 178, 20), from=100, to=250)
# plot priors
curve(dunif(x, 0, 50), from=-10, to=60)
# so 50 sd assume 95% of individual heights lie within 100cm of average height - large range
precis(m4.1)
cat(mean(d2$height),sd(d2$height))

flist <- 
m4.2 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 0.1),
    sigma ~ dunif(0, 50)
  ), 
  data=d2)

# plot priors
curve(dnorm(x, 178, 20), from=100, to=250)
curve(dnorm(x, 178, .1), from=100, to=250, add=TRUE)
# plot priors
curve(dunif(x, 0, 50), from=-10, to=60)
# so 50 sd assume 95% of individual heights lie within 100cm of average height - large range
precis(m4.1)
precis(m4.2)
cat(mean(d2$height),sd(d2$height))


```
Can sample from priors ...
```{r}
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
gauss_h <- rnorm(1e4, 178, 25)
dens(gauss_h, add=TRUE, col='red')
```

initial starting condition for map search
```{r}
start <- list(mu=mean(d2$height), sigma=sd(d2$height))
```

sampling from map - just multivariate gaussian
variance cobvariance

```{r}
vcov(m4.1)
diag(vcov(m4.1))
cov2cor(vcov(m4.1))
post <- extract.samples(m4.1, n=1e4)
head(post)
precis(post)
```

Getting $\sigma$ right. quadratic assumption for sigma can be problematic. conventional approach take log
```{r}
m4.1_logsigma <- map(
  alist(
    height ~ dnorm(mu, exp(log_sigma)),
    mu ~ dnorm(178, 0.1),
    log_sigma ~ dnorm(2, 10)
  ), 
  data=d2)

post <- extract.samples(m4.1_logsigma) # default 10000
sigma <- exp(post$log_sigma)
```

## Adding a predictor
```{r}

```

params like a are intercepts, and are meaningless in isolation from other parameters (a person of weight zero has height 113)
sigma - 95% of heights lie within +/- 10 cm.
note almost perfect negative correlation.. This can cause problems in estimation
```{r}
m4.3 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <-  a + b*weight,
    a ~ dnorm(156, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), 
  data=d2)

# show correlation matrix too
precis(m4.3, corr=TRUE)
plot(d2$height ~ d2$weight, col="blue")
abline(a=coef(m4.3)["a"], b=coef(m4.3)["b"])
post <- extract.samples(m4.3) 
```

intervals
```{r}
mu <- link(m4.3)
str(mu)
# 1000 rows (samples) by 352 columns=individuals - distribution of mu
weight.seq <- seq(from=25, to=70, by=1)
mu <- link(m4.3, data=data.frame(weight=weight.seq))
str(mu)
#use type="n" to hide raw data
plot(height ~ weight, d2, type="n")
for (i in 1:100){
  points(weight.seq, mu[i,], pch=16, col=col.alpha(rangi2,0.1))
}
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)
plot(height ~ weight, d2, col=col.alpha(rangi2,0.5))
lines(weight.seq, mu.mean)
shade(mu.HPDI, weight.seq)


```

```{r}

N <- 100
height <- rnorm(N, 10, 2)
leg_prop <- runif(N, 0.4, 0.5)
leg_left <- leg_prop * height + rnorm(N, 0, 0.02)
leg_right <- leg_prop * height + rnorm(N, 0, 0.02)
d <- data.frame(height, leg_left, leg_right)
```

```{r}
m5.8 <- map(
  alist(
    height ~dnorm(mu, sigma),
    mu <- a + bl *leg_left + br * leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 1),
    br ~ dnorm(2, 1),
    sigma ~ dunif(0, 10)
  ),
  data=d)
precis(m5.8)
```
Use penalised ML with glmnet.
the regularisation defaults much stronger.  How is this identified?
(but then crossvalidate) .. and might not identify minimum in section

Cannot set this up as a random effects model

```{r}
library(glmnet)
library(lme4)
#lmer(height ~ leg_left|1,data=d)

X <- as.matrix(d[,c("leg_left", "leg_right")])
y <- as.matrix(d$height)
md <- cv.glmnet(X, y, alpha=0, standardize=FALSE, lambda = seq(0.0001,0.01,length.out=1000))
cat(md$lambda.min, md$lambda.1se,coef(m5.8)["sigma"])
sigma_b <- coef(m5.8)["sigma"]/sqrt(min(md$lambda *N))
sds <- sqrt(diag(vcov(m5.8)))
lambda <- (coef(m5.8)["sigma"]/sds["bl"])^2/N
md$glmnet.fit$beta


```

 

log likelihood of single point is $(\frac{\epsilon^2}{2\sigma^2})$

prior is $\frac{\alpha^2}{2\sigma_\alpha^2}$
therefore we have 
 $  mse + \frac {\sigma^2}{N\sigma_\alpha^2}|\alpha|^2$
 
 ie the lambda in glmnet is given by above expression
 
Using factors as indices to coefficients.  Need to explicitly specify intercept term. and need quite a lot of regularisation to ensure a_0 captures average?
could not match
```{r }
data(milk)
d <-milk
unique(d$clade)
(d$clade_id <- coerce_index(d$clade))

X <- model.matrix(~ factor(clade_id) + 0, d)
y <-  as.matrix(d$kcal.per.g)
md <- cv.glmnet(X,y, standardize=FALSE, alpha=0)
lambda = md$lambda.min #min(md$lambda)

lambda_idx <- which(md$lambda==lambda)[1]
N <- nrow(X)
p <- ncol(X)
sigma_tr <- sqrt(sum((predict(md, s=lambda, newx=X) -y)^2)/(N-p))
sigma_te <- sqrt(md$cvm[lambda_idx])
sigma_b <- sqrt(md$cvm[lambda_idx]/(lambda * N))


m5.16_alt = map(
alist(
  kcal.per.g ~ dnorm(mu, sigma),
  mu <- a[clade_id],
  a[clade_id] ~ dnorm(.6, 10),
  sigma ~ dunif(0,10)
),
data=d
)

m5.16_alt_reg = map(
alist(
  kcal.per.g ~ dnorm(mu, sigma),
  mu <- a[clade_id] +a_0,
  a_0 ~ dnorm(0, 10),
  a[clade_id] ~ dnorm(0, .4),
  sigma <- 0.12  #~ dunif(0,10)
),
data=d
)
precis(m5.16_alt, depth=2)
precis(m5.16_alt_reg, depth=2)

coef(md, s=lambda)
coef(m5.16_alt_reg)
```

```{r}
library(rethinking)
data(reedfrogs)
d <- reedfrogs

d$tank <- 1:nrow(d)
dat <- list(
n_tanks = nrow(d),
S = d$surv,
N = d$density,
tank = d$tank )
str(d)
X <- model.matrix( ~ as.factor(tank) + 0, data=d)
y <- as.matrix(cbind(d$density - d$surv, d$surv))
n_tanks <- nrow(d)
d_exp1 <- data.frame(tank=as.factor(d$tank), N=d$density, S=d$surv)
al <-  vector("list", n_tanks)
for (i in seq(n_tanks)){
  print(i)
  al[[i]] <- data.frame(tank=rep(d_exp1[i, 'tank'], d_exp1[i, 'N']), S=c(rep(1, d_exp1[i, 'S']), rep(0, d_exp1[i, 'N'] - d_exp1[i, 'S'])))
}
d_expand <- do.call(rbind,al)
X_expand <- model.matrix( ~ as.factor(tank) + 0, data=d_expand)
y_expand <- as.matrix(d_expand$S)

#dup <- function(x){data.frame(tank=rep(x[[1]], x[[2]]))}#,S=rbind(rep(1, x[[3]]), rep(0, x[[2]] -x[[3]])))}


```



```{r}
mod <- cv.glmnet(X,y, family='binomial', alpha=0)
plot(mod)
mod_expand <- cv.glmnet(X_expand,y_expand, family='binomial', alpha=0)
plot(mod_expand)

```

```{stan output.var="tadpoles"}

data {
  int<lower=0> n_tanks;          // number of schools 
  int S[n_tanks];               // survival
  int N[n_tanks];               // original
  
}

parameters {
  real a_bar;
  real a[n_tanks]; 
  real<lower=0> sigma[n_tanks];  // s.e. of effect estimates 
  vector[n_tanks] eta;
}

model {
  target += normal_lpdf(a_bar| 0, 1.5);
  target += normal_lpdf(a | a_bar, sigma);
  target += binomial_logit_lpmf(S | N, a);
}

```

```{r}
tadpoles_fit <- rstan::sampling(tadpoles, data = dat, chains=4 )
```

# Statistics in Action

```{r}
data("Orthodont", package="nlme")
head(Orthodont)
```

```{r}
library(ggplot2)
theme_set(theme_bw())
pl <- ggplot(data=Orthodont) + geom_point(aes(x=age,y=distance), color="red", size=3) 
pl
```
age only model
```{r}
lm1 <- lm(distance~age, data=Orthodont)
summary(lm1)
```


```{r}
pl + geom_line(aes(x=age,y=predict(lm1)))
```

investigate boys and girls separately
```{r}
pl + geom_line(aes(x=age,y=predict(lm1))) + facet_grid(.~ Sex )
```


```{r}
lm2 <- lm(distance~age+Sex, data=Orthodont)
summary(lm2)
Orthodont$pred.lm2 <- predict(lm2)
pl + geom_line(data=Orthodont,aes(x=age,y=pred.lm2)) + facet_grid(.~ Sex )
```
We could instead assume the same intercept but different slopes for boys and girls:

```{r}
lm3 <- lm(distance~age:Sex , data=Orthodont)
summary(lm3)
Orthodont$pred.lm3 <- predict(lm3)
pl + geom_line(data=Orthodont,aes(x=age,y=pred.lm3)) + facet_grid(.~ Sex )
```


```{r}
lm4 <- lm(distance~age:Sex+Sex, data=Orthodont)
summary(lm4)
Orthodont$pred.lm4 <- predict(lm4)
pl + geom_line(data=Orthodont,aes(x=age,y=pred.lm4)) + facet_grid(.~ Sex )
```

*The p-value cannot be used as such since the design matrix is not orthogonal*
```{r}
C <- t(model.matrix(lm4))%*%model.matrix(lm4)
C/sqrt(diag(C)%*%t(diag(C)))
```

```{r}
summary(lm(distance ~ age , data=subset(Orthodont, Sex=="Male")))
```

 Let us look at the individual fits for 8 subjects,
```{r}
Subject.select <- c(paste0("M0",5:8),paste0("F0",2:5))
Orthodont.select <- subset(Orthodont,Subject %in% Subject.select)
ggplot(data=Orthodont.select) + geom_point(aes(x=age,y=distance), color="red", size=3) + 
  geom_line(aes(x=age,y=predict(lm3,newdata=Orthodont.select))) + facet_wrap(~Subject, nrow=2) 
```
We see that the model for the boys, respectively for the girls, seems to underestimate or overestimate the individual data of the four boys, respectively the four girls.

Indeed, we didnâ€™t take into account the fact that the data are repeated measurements made on the same subjects. A more convenient plot for this type of data consists in joining the data of a same individual:

```{r}
library(ggplot2)
theme_set(theme_bw())
ggplot(data=Orthodont) + geom_point(aes(x=age,y=distance), color="red", size=3)  + 
  geom_line(aes(x=age,y=distance,group=Subject)) # +  facet_grid(~Sex) 

```
##  Fitting linear mixed effects models to the orthodont data
### Fitting a first model

A first linear mixed effects model assumes that the birth distance and the growth rate (i.e. the intercept and the slope) may depend on the individual:

We can use the function lmer for fitting this model. By default, the restricted mximum likelihood (REML) method is used.
```{r}
library(lme4)
lmem <- lmer(distance ~ age + (age|Subject), data = Orthodont) 
summary(lmem)
```
Note that functions fixef and VarCorr return these estimated parameters:

```{r}
(psi.pop <- fixef(lmem))
```
```{r}
(Omega <- VarCorr(lmem)$Subject[,])
```

```{r}
(sigma2 <- attr(VarCorr(lmem), "sc")^2)
```

The estimated individual parameters for our 8 selected individuals can be obtained using function coef


```{r}
Orthodont.i <- Orthodont[Orthodont$Subject=="M05",]
yi <- Orthodont.i$distance
Ai <- cbind(1,Orthodont.i$age)
iO <- solve(Omega)
Gammai <- solve(t(Ai)%*%Ai/sigma2 + iO)
mui <- Gammai%*%(t(Ai)%*%yi/sigma2 + iO%*%psi.pop)
mui
```


Individual predicted distances can also be computed and plotted with the observed distances

```{r}

Orthodont$pred.lmem <- fitted(lmem)
ggplot(data=subset(Orthodont,Subject %in% Subject.select)) + geom_point(aes(x=age,y=distance), color="red", size=3) + 
  geom_line(aes(x=age,y=pred.lmem)) + facet_wrap(~Subject, ncol=4) 
```
